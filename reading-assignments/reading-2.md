### Response
Technical tools are considered “objective” because they are designed to output the optimal solution according to the given data and algorithm. In other words, what you feed, what you get. In this sense, some obvious prejudices and discriminations to human can be somehow avoided by adjusting the variety of input dataset, the features, or the chosen algorithm. Yet what about those discriminations even hidden to human’s eyes at one time? Chances are that we humans don’t even recognize all the contributing factors of the problem, let alone the machines could be programmed to learn from the data and eliminate all the bias. Machines are initialized under human guidance so far, therefore bias is destined to exist in terms of such mechanism. According to Virginia Eubanks, once machines and technical tools created the “triage model”, they are actually stating a priority. Instead of fairing out discriminations in social/welfare systems, they are essentially allocating resources according to this priority. Discriminations, therefore, would never be eliminated unless the priority is absolutely objective and fair, which is almost impossible to set under the complex social contexts and individual differences.
In this sense, from the terms “the state doesn’t need a cop to kill a person” and “electronic incarceration”, I can only see the expectations on avoiding cops’ or prison guards’ mistakes. But who can kill a person and guard the incarceration then? Is it more a question about “who should” or about “why would”?

One similar example that I can think of is [“social credit” monitoring](https://www.youtube.com/watch?v=GsIdUGWsXn8&feature=emb_title). Zhima Credit (also known as Sesame Credit), is also a program run by Ant Financial (Alibaba). It measures customer trustworthiness according to their online behavior and credit history, and offers benefits including low-interest loans, preferential access to bike and car-sharing services, etc to those with high credit scores. In some cities where local government instituted mandatory pilot social credit systems, “redlists” are set to encourage trustworthy behavior among individuals, businesses, and even government departments. Accordingly, blacklists exist for those committing minor infractions and behave untrustworthily, and punishment includes but is not limited to public sector employment limitations and government benefits refusal. Though the overall system seems to strike a balance between both citizens’ and organizations’ sides and ensures their credits for the sake of societal processes, whether the data are shared with authorities, whether “blacklists” deprives individual rights and liberty and other debates still requires emphasis.

### Materials
##### Listen
- [Virginia Eubanks, Automating Inequality with ](https://www.writersvoice.net/tag/virginia-eubanks/) by Writer's Voice Podcast (2019)
- [AI: The Problem with Bias, with Kate Crawford](https://open.spotify.com/episode/0ysGO67iXaPmTx4h9v33z3?si=FmJeEuyJTeiqckjpHCTlVQ), City Arts & Lectures Podcast (2018)

##### Watch
- [Machine Learning and Human Bias](https://www.youtube.com/watch?v=59bMh59JQDo), Video by Google (2017)
- [How I'm fighting bias in algorithms](https://www.youtube.com/watch?v=UG_X_7g63rY), Ted Talk by Joy Buolamwini (2017)
- [Invisible Images Of Surveillance](https://www.youtube.com/watch?v=ijVTdSoZEC4), Trevor Paglen (2018)

### Prompts
>How the technical tools promise to *fair out* the remaining discrimination that exist in social/welfare systems? In how far can they succeed, in which ways do they fail?

>Imagine, what could this (following quote) mean in the widest sense?
"*The state doesn't need a cop to kill a person" and "electronic incarceration*"

>What do you understand this to mean?
"*systems act as a kind of 'empathy-overwrite'*"

>China is much more advanced and expansive when it comes to applying technical solutions to societal processes or instant challenges ([recent example](https://www.nytimes.com/2020/03/01/business/china-coronavirus-surveillance.html?)). Try to point example cases in China that are in accordance or in opposition to the problematics discussed in the podcast. Perhaps you can think of
"*technical systems not well thought-through about what their impact on human beings is*"
